# 🔧 专家LLM实时调试功能说明

## 功能概述

为了方便用户和开发者了解专家LLM的工作状态，我们添加了实时调试功能，可以在专家LLM对话框中实时显示程序和专家LLM正在进行的工作。

## 实现的功能

### 1. 后端调试信息 ✅

在`main.py`的WebSocket端点中添加了详细的调试信息发送：

```python
# 发送调试状态信息的函数
async def send_debug(message, status="info"):
    try:
        await websocket.send_json({"debug": message, "status": status, "timestamp": time.time()})
    except Exception as e:
        logger.error(f"发送调试信息失败: {str(e)}")

# 调试信息示例
await send_debug(f"🚀 开始处理专家LLM查询: {query[:50]}...")
await send_debug(f"📋 使用展板ID: {board_id}")
await send_debug(f"🔧 专家LLM实例: {type(expert).__name__}")
await send_debug(f"📝 会话ID: {expert.session_id}")
await send_debug("🔄 准备调用流式LLM...")
await send_debug("✅ _prepare_messages方法存在")
await send_debug("📡 开始流式生成...")
await send_debug(f"✅ 流式生成完成，响应长度: {len(full_response)}")
```

### 2. 专家LLM内部调试 ✅

在`expert_llm.py`的`stream_call_llm`方法中添加了详细的日志：

```python
logger.info(f"🔥 stream_call_llm 开始执行 - 展板: {self.board_id}")
logger.info("🔍 检查 _prepare_messages 方法...")
logger.info("✅ _prepare_messages 方法存在，开始获取会话历史...")
logger.info(f"📝 准备了 {len(messages)} 条消息")
logger.info("🌐 开始流式API请求...")
logger.info(f"📡 HTTP响应状态: {response.status_code}")
logger.info(f"📦 收到数据块 {chunk_count}: '{content}' (长度: {len(content)})")
logger.info(f"🔄 调用回调函数，内容: '{content}'")
logger.info("✅ 回调函数调用成功")
logger.info(f"📊 流式处理统计 - 总块数: {chunk_count}, 总长度: {len(full_response)}")
```

### 3. 前端调试信息显示 ✅

在`BoardExpertPanel.js`中添加了调试信息的接收和显示：

```javascript
// 处理调试信息 - 在对话框中显示
if (data.debug) {
  const debugMessage = {
    role: 'system',
    content: `🔧 [调试] ${data.debug}`,
    isDebug: true,
    timestamp: new Date().toISOString()
  };
  
  setMessages(prev => [...prev, debugMessage]);
}
```

### 4. 调试消息样式 ✅

为调试消息设计了专门的显示样式：

- **颜色**: 浅绿色背景 (`#f6ffed`)
- **边框**: 绿色边框 (`#b7eb8f`) 
- **字体**: 等宽字体 (`monospace`)
- **大小**: 较小字体 (12px)
- **间距**: 较小间距 (8px)
- **透明度**: 80% 透明度

## 调试信息类型

### 🚀 开始状态
- `🚀 开始处理专家LLM查询: {query}`
- `📋 使用展板ID: {board_id}`
- `🔧 专家LLM实例: ExpertLLM`
- `📝 会话ID: {session_id}`

### 🔄 处理过程
- `🔄 准备调用流式LLM...`
- `✅ _prepare_messages方法存在`
- `📡 开始流式生成...`
- `🌐 开始流式API请求...`
- `📡 HTTP响应状态: 200`

### 📦 流式数据
- `📦 收到数据块 {count}: '{content}' (长度: {length})`
- `🔄 调用回调函数，内容: '{content}'`
- `✅ 回调函数调用成功`

### ✅ 完成状态
- `✅ 流式生成完成，响应长度: {length}`
- `📊 流式处理统计 - 总块数: {count}, 总长度: {length}`

### ❌ 错误信息
- `❌ _prepare_messages方法不存在`
- `❌ 流式生成失败: {error}`
- `❌ 回调函数执行失败: {error}`

## 使用方法

1. **打开专家LLM面板**: 点击展板上的专家LLM图标
2. **发送查询**: 输入任何问题并发送
3. **观察调试信息**: 在对话框中会实时显示绿色的调试消息
4. **查看处理过程**: 从查询开始到完成的整个过程都会有调试信息

## 示例调试输出

用户发送："告诉我4开头那个PDF文件第4页的内容"

调试信息显示：
```
🔧 [调试] 🚀 开始处理专家LLM查询: 告诉我4开头那个PDF文件第4页的内容...
🔧 [调试] 📋 使用展板ID: file-course-9-3
🔧 [调试] 🔧 专家LLM实例: ExpertLLM
🔧 [调试] 📝 会话ID: expert_file-course-9-3_xxx-xxx-xxx
🔧 [调试] 🔄 准备调用流式LLM...
🔧 [调试] ✅ _prepare_messages方法存在
🔧 [调试] 📡 开始流式生成...
🔧 [调试] ✅ 流式生成完成，响应长度: 204
```

## 技术细节

### WebSocket消息格式

调试信息通过WebSocket发送，格式为：
```json
{
  "debug": "🚀 开始处理专家LLM查询: ...",
  "status": "info",
  "timestamp": 1674567890.123
}
```

### 状态类型
- `info`: 一般信息 (默认)
- `success`: 成功状态
- `error`: 错误状态  
- `warning`: 警告状态

## 优势

1. **实时反馈**: 用户可以实时了解系统工作状态
2. **问题诊断**: 开发者可以快速定位问题
3. **透明度**: 用户了解AI处理过程，增加信任度
4. **调试便利**: 无需查看后台日志即可了解处理状态

## 下一步改进

1. **调试开关**: 允许用户开启/关闭调试信息显示
2. **详细程度**: 提供不同级别的调试信息
3. **性能监控**: 显示处理时间和性能指标
4. **错误恢复**: 在出现错误时提供恢复建议

## 状态确认

✅ **专家LLM不再报错**: `'ExpertLLM' object has no attribute '_prepare_messages'` 问题已解决
✅ **调试信息正常显示**: 前端可以实时看到处理状态
✅ **WebSocket连接稳定**: 连接和消息传输正常
✅ **用户体验改善**: 用户可以了解系统工作状态

现在用户可以在专家LLM对话框中看到完整的处理过程，既满足了调试需求，也提高了用户体验！ 