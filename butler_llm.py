import os
import json
import logging
import requests
import uuid
import time
import datetime
from config import QWEN_API_KEY, API_TIMEOUT
from board_logger import board_logger
from conversation_manager import conversation_manager
import expert_llm
import llm_agents  # å¯¼å…¥LLMäº¤äº’æ¨¡å—
from typing import Dict, List, Any, Optional, Union
from llm_logger import LLMLogger  # å¯¼å…¥LLMæ—¥å¿—è®°å½•å™¨

logger = logging.getLogger(__name__)

class ButlerLLM:
    """
    ç®¡å®¶LLMï¼Œè´Ÿè´£å…¨å±€æ“ä½œå’Œå¤šå±•æ¿åè°ƒ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç®¡å®¶LLM"""
        # åˆ›å»ºç®¡å®¶LLMçš„ç‹¬ç«‹ä¼šè¯ID
        self.session_id = f"butler_{uuid.uuid4()}"
        self.butler_log_file = "butler_log.json"
        
        # æ·»åŠ å¤šæ­¥æ“ä½œçŠ¶æ€è¿½è¸ª - ç¡®ä¿åœ¨ä»»ä½•æ–¹æ³•è°ƒç”¨å‰åˆå§‹åŒ–
        self.multi_step_context = {
            "active": False,
            "task": None,
            "plan": None,
            "steps": [],
            "commands": [],
            "current_step": 0,
            "previous_result": None,
            "results": []
        }
        
        # åˆå§‹åŒ–ç®¡å®¶æ—¥å¿—
        self._init_butler_log()
        
        # åˆå§‹åŒ–ç®¡å®¶å¯¹è¯
        self._init_butler_conversation()
    
    def _init_butler_log(self):
        """åˆå§‹åŒ–ç®¡å®¶æ—¥å¿—"""
        if os.path.exists(self.butler_log_file):
            try:
                with open(self.butler_log_file, 'r', encoding='utf-8') as f:
                    self.butler_log = json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½ç®¡å®¶æ—¥å¿—å¤±è´¥: {str(e)}")
                self._create_new_butler_log()
        else:
            self._create_new_butler_log()
    
    def _create_new_butler_log(self):
        """åˆ›å»ºæ–°çš„ç®¡å®¶æ—¥å¿—"""
        self.butler_log = {
            "app_state": "initialized",
            "file_structure": {},
            "boards": {},
            "user_preferences": {},
            "recent_operations": []
        }
        self._save_butler_log()
    
    def _save_butler_log(self):
        """ä¿å­˜ç®¡å®¶æ—¥å¿—"""
        try:
            with open(self.butler_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.butler_log, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ç®¡å®¶æ—¥å¿—å¤±è´¥: {str(e)}")
    
    def _init_butler_conversation(self):
        """åˆå§‹åŒ–ç®¡å®¶å¯¹è¯"""
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._get_system_prompt()
        
        # åˆå§‹åŒ–å¯¹è¯
        conversation_manager.add_message(
            self.session_id, 
            "global", 
            "system", 
            system_prompt
        )
        
        # æ·»åŠ åˆå§‹åŒ–æ¶ˆæ¯ï¼Œä½†ä¸å®é™…è°ƒç”¨API
        conversation_manager.add_message(
            self.session_id,
            "global",
            "user",
            "åˆå§‹åŒ–ç®¡å®¶LLMï¼Œè¯·æä¾›åº”ç”¨æ¦‚è§ˆã€‚"
        )
        
        # æ·»åŠ æ¨¡æ‹Ÿçš„åˆå§‹å“åº”ï¼Œé¿å…åˆå§‹åŒ–æ—¶è°ƒç”¨API
        init_response = "WhatNoteå·²å¯åŠ¨ï¼Œç®¡å®¶LLMåˆå§‹åŒ–å®Œæˆã€‚"
        conversation_manager.add_message(
            self.session_id,
            "global",
            "assistant",
            init_response
        )
        
        # è®°å½•åˆ°ç®¡å®¶æ—¥å¿—
        self.add_operation("butler_initialized", {"session_id": self.session_id})
    
    def _get_system_prompt(self):
        """è·å–ç®¡å®¶LLMçš„ç³»ç»Ÿæç¤ºè¯"""
        # è·å–å½“å‰æ–‡ä»¶ç»“æ„ä¿¡æ¯
        file_structure_summary = self.butler_log.get("file_structure_summary", {})
        course_folders = file_structure_summary.get("course_folder_list", [])
        pdf_files = file_structure_summary.get("pdf_list", [])
        
        # æ„å»ºæ–‡ä»¶ç»“æ„æè¿°
        file_structure_description = ""
        if course_folders or pdf_files:
            file_structure_description = "\n\nã€å½“å‰æ–‡ä»¶ç»“æ„ä¿¡æ¯ã€‘\n"
            if course_folders:
                file_structure_description += f"è¯¾ç¨‹æ–‡ä»¶å¤¹: {', '.join(course_folders)}\n"
            if pdf_files:
                file_structure_description += f"PDFæ–‡ä»¶: {', '.join(pdf_files)}\n"
        
        base_prompt = """ä½ æ˜¯WhatNoteåº”ç”¨çš„ç®¡å®¶LLMï¼Œè´Ÿè´£å…¨å±€æ“ä½œå’Œå¤šå±•æ¿åè°ƒã€‚

ä½ çš„ä¸»è¦èŒè´£åŒ…æ‹¬ï¼š
1. ç®¡ç†æ–‡ä»¶ç»“æ„ï¼ˆè¯¾ç¨‹æ–‡ä»¶å¤¹ã€ç« èŠ‚å±•æ¿ï¼‰
2. åè°ƒå„å±•æ¿çš„ä¸“å®¶LLM
3. å¤„ç†è·¨å±•æ¿çš„æ“ä½œè¯·æ±‚
4. å›ç­”ç”¨æˆ·å…³äºåº”ç”¨ä½¿ç”¨çš„é—®é¢˜
5. æ‰§è¡Œç”¨æˆ·è¯·æ±‚çš„å¤æ‚ä»»åŠ¡

ä½ æ‹¥æœ‰ä»¥ä¸‹èƒ½åŠ›:
1. æŸ¥çœ‹å’Œä¿®æ”¹æ–‡ä»¶ç»“æ„
2. åˆ›å»ºå’Œç®¡ç†å±•æ¿
3. ä¸ä¸“å®¶LLMé€šä¿¡
4. æ‰§è¡Œå¤šæ­¥éª¤æ“ä½œï¼Œä½†æ¯æ­¥æ“ä½œå‰éœ€è·å¾—ç”¨æˆ·ç¡®è®¤

æ‰§è¡Œæ“ä½œæ—¶ï¼Œä½ åº”è¯¥ï¼š
1. åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œç¡®å®šæ‰€éœ€çš„æ“ä½œæ­¥éª¤
2. æä¾›æ˜ç¡®çš„æ“ä½œè®¡åˆ’
3. ç­‰å¾…ç”¨æˆ·ç¡®è®¤åæ‰§è¡Œæ¯ä¸€æ­¥
4. æ ¹æ®æ“ä½œç»“æœè°ƒæ•´åç»­æ­¥éª¤
5. è®°ä½å‰åºå‘½ä»¤çš„æ‰§è¡Œç»“æœï¼Œä¿æŒæ“ä½œè¿è´¯æ€§

å¤šæ­¥æ“ä½œæ—¶ï¼Œä½ éœ€è¦ï¼š
1. æ¸…æ™°è®°ä½å½“å‰å¤„äºå“ªä¸€æ­¥
2. æ¯ä¸€æ­¥çš„æ‰§è¡Œç»“æœè¦ä¼ é€’ç»™ä¸‹ä¸€æ­¥
3. åœ¨ç”¨æˆ·ç¡®è®¤å®Œä¸€ä¸ªæ­¥éª¤åï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥
4. æ ¹æ®å‰åºæ­¥éª¤ç»“æœåŠ¨æ€è°ƒæ•´åç»­æ­¥éª¤è®¡åˆ’

è¯·ä¿æŒä¸“ä¸šã€é«˜æ•ˆã€å‹å¥½çš„æ€åº¦ï¼ŒååŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ã€‚"""
        
        # ç»„åˆåŸºç¡€æç¤ºå’Œæ–‡ä»¶ç»“æ„ä¿¡æ¯
        return base_prompt + file_structure_description
    
    def add_operation(self, operation_type, data=None):
        """æ·»åŠ æ“ä½œåˆ°ç®¡å®¶æ—¥å¿—"""
        if "recent_operations" not in self.butler_log:
            self.butler_log["recent_operations"] = []
            
        import datetime
        operation = {
            "type": operation_type,
            "timestamp": datetime.datetime.now().isoformat(),
            "data": data or {}
        }
        
        self.butler_log["recent_operations"].append(operation)
        
        # é™åˆ¶æ“ä½œå†å²è®°å½•æ•°é‡
        if len(self.butler_log["recent_operations"]) > 100:
            self.butler_log["recent_operations"] = self.butler_log["recent_operations"][-100:]
            
        self._save_butler_log()
    
    def update_file_structure(self, file_structure):
        """æ›´æ–°æ–‡ä»¶ç»“æ„ä¿¡æ¯"""
        # ä¿å­˜å®Œæ•´çš„æ–‡ä»¶ç»“æ„
        self.butler_log["file_structure"] = file_structure
        
        # åˆ›å»ºç®€åŒ–ç‰ˆä¿¡æ¯ç”¨äºLLMæç¤º
        summary = {
            "course_folders": len(file_structure.get("course_folders", [])),
            "boards": len(file_structure.get("boards", [])),
            "uploaded_files": len(file_structure.get("uploaded_files", [])),
            "course_folder_list": [folder.get("name") for folder in file_structure.get("course_folders", [])],
            "pdf_list": [file.get("filename") for file in file_structure.get("uploaded_files", [])]
        }
        
        self.butler_log["file_structure_summary"] = summary
        self._save_butler_log()
        
        # è®°å½•æ“ä½œ
        self.add_operation("file_structure_updated", {
            "course_folders": len(file_structure.get("course_folders", [])),
            "boards": len(file_structure.get("boards", [])),
            "uploaded_files": len(file_structure.get("uploaded_files", []))
        })
    
    def update_board_info(self, board_id):
        """æ›´æ–°ç‰¹å®šå±•æ¿çš„ä¿¡æ¯"""
        board_summary = board_logger.get_board_summary(board_id)
        
        if "boards" not in self.butler_log:
            self.butler_log["boards"] = {}
            
        self.butler_log["boards"][board_id] = board_summary
        self._save_butler_log()
        
        # è®°å½•æ“ä½œ
        self.add_operation("board_info_updated", {"board_id": board_id})
    
    def process_user_request(self, request, status_log=None):
        """
        å¤„ç†ç”¨æˆ·è¯·æ±‚
        
        Args:
            request: ç”¨æˆ·è¯·æ±‚å†…å®¹
            status_log: å½“å‰åº”ç”¨çŠ¶æ€æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»“æœå’Œå¯èƒ½çš„æ“ä½œå‘½ä»¤
        """
        # åˆå§‹åŒ–function callsè®°å½•
        self.last_function_calls = []
        
        # æ„å»ºæç¤ºè¯
        prompt = f"ã€ç”¨æˆ·è¯·æ±‚ã€‘{request}\n\n"
        
        # æ·»åŠ çŠ¶æ€ä¿¡æ¯
        if status_log:
            prompt += f"å½“å‰åº”ç”¨çŠ¶æ€:\n{status_log}\n\n"
            
        # æ·»åŠ å¯ç”¨çš„æ“ä½œå‘½ä»¤æç¤º
        prompt += """ä½ å¯ä»¥æ‰§è¡Œä»¥ä¸‹ç±»å‹çš„æ“ä½œ:
1. å¯¼èˆªæ“ä½œ: next_page, prev_page, goto_page
2. çª—å£æ“ä½œ: open_window, close_window, close_all
3. å†…å®¹ç”Ÿæˆ: generate_note, generate_annotation, vision_annotate
4. æ–‡ä»¶æ“ä½œ: select_pdf, upload_pdf, create_course_folder, delete_file
5. å±•æ¿æ“ä½œ: create_board, open_board, close_board, list_boards
6. ä¸ä¸“å®¶LLMäº¤äº’: consult_expert
7. å¤šæ­¥ä»»åŠ¡: plan_task, execute_step
8. ç³»ç»ŸæŸ¥è¯¢: get_app_state, get_file_list, get_board_info

å¦‚æœéœ€è¦æ‰§è¡Œæ“ä½œï¼Œè¯·åœ¨å›å¤ä¸­åŒ…å«JSONæ ¼å¼çš„æ“ä½œå‘½ä»¤ã€‚
ä¾‹å¦‚: {"type": "navigation", "action": "next_page"}
æˆ–å¸¦å‚æ•°çš„: {"type": "navigation", "action": "goto_page", "params": {"page": 5}}

å¦‚æœç”¨æˆ·è¯·æ±‚éœ€è¦å¤šæ­¥æ“ä½œï¼Œè¯·ä½¿ç”¨plan_taskå‘½ä»¤è§„åˆ’ä»»åŠ¡ã€‚
ä¾‹å¦‚: {"type": "task", "action": "plan_task", "params": {"task": "åˆ›å»ºå­¦ä¹ è®¡åˆ’"}}

å¦‚æœä¸éœ€è¦æ‰§è¡Œæ“ä½œï¼Œç›´æ¥æä¾›ä¿¡æ¯æ€§å›å¤å³å¯ã€‚"""
        
        # è°ƒç”¨LLM
        response = self._call_llm(prompt)
        
        # è®°å½•æ“ä½œ
        self.add_operation("user_request_processed", {
            "request_preview": request[:50] + "..." if len(request) > 50 else request
        })
        
        # å°è¯•ä»å›å¤ä¸­æå–æ“ä½œå‘½ä»¤
        command = self._extract_command_json(response)
        
        # å¦‚æœæœ‰å‘½ä»¤ï¼Œå°è¯•æ‰§è¡Œfunction call
        if command:
            try:
                function_result = self._execute_function_call(command)
                self.last_function_calls.append({
                    "function": command.get("action"),
                    "args": command.get("params", {}),
                    "result": function_result,
                    "status": "completed"
                })
            except Exception as e:
                logger.error(f"Function callæ‰§è¡Œå¤±è´¥: {str(e)}")
                self.last_function_calls.append({
                    "function": command.get("action"),
                    "args": command.get("params", {}),
                    "result": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                    "status": "failed"
                })
        
        return {
            "response": self._clean_response_json(response),
            "command": command
        }
    
    def _extract_command_json(self, response):
        """ä»å›å¤ä¸­æå–JSONæ ¼å¼çš„æ“ä½œå‘½ä»¤"""
        import re
        import json
        
        # å°è¯•å¯»æ‰¾JSONæ ¼å¼çš„å‘½ä»¤
        json_pattern = r'({[\s\S]*?})'
        json_matches = re.findall(json_pattern, response)
        
        for match in json_matches:
            try:
                cmd = json.loads(match)
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å‘½ä»¤å­—æ®µ
                if isinstance(cmd, dict) and "type" in cmd and "action" in cmd:
                    return cmd
            except:
                pass
                
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONå‘½ä»¤ï¼Œè¿”å›None
        return None
    
    def _clean_response_json(self, response):
        """æ¸…ç†å›å¤ï¼Œç§»é™¤JSONå‘½ä»¤éƒ¨åˆ†"""
        import re
        
        # ç§»é™¤JSONæ ¼å¼çš„å‘½ä»¤
        cleaned = re.sub(r'```json\s*({[\s\S]*?})\s*```', '', response)
        cleaned = re.sub(r'({[\s\S]*?})', '', cleaned)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    def consult_expert(self, board_id, question, context=None):
        """
        å’¨è¯¢ä¸“å®¶LLM
        
        Args:
            board_id: å±•æ¿ID
            question: é—®é¢˜å†…å®¹
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¸“å®¶LLMçš„å›å¤
        """
        # è·å–å±•æ¿çš„ä¸“å®¶LLM
        from expert_llm import ExpertLLMRegistry
        expert = ExpertLLMRegistry.get_or_create(board_id)
        
        if not expert:
            error_msg = f"æ— æ³•è·å–å±•æ¿ {board_id} çš„ä¸“å®¶LLM"
            logger.error(error_msg)
            return error_msg
        
        # æ·»åŠ å’¨è¯¢å‰ç¼€å’Œä¸Šä¸‹æ–‡
        expert_prompt = f"ã€ç®¡å®¶LLMå’¨è¯¢ã€‘{question}"
        
        if context:
            expert_prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context}"
        
        # è·å–ä¸“å®¶å›å¤
        expert_response = expert.process_user_message(expert_prompt)
        
        # è®°å½•æ“ä½œ
        self.add_operation("expert_consulted", {
            "board_id": board_id,
            "question_preview": question[:50] + "..." if len(question) > 50 else question
        })
        
        return expert_response
    
    def plan_multi_step_task(self, task_description, context=None):
        """
        è§„åˆ’å¤šæ­¥éª¤ä»»åŠ¡
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä»»åŠ¡è®¡åˆ’
        """
        # æ„å»ºæç¤ºè¯
        prompt = f"ã€å¤šæ­¥éª¤ä»»åŠ¡è§„åˆ’ã€‘ç”¨æˆ·ä»»åŠ¡: {task_description}\n\n"
        
        if context:
            prompt += f"ä»»åŠ¡ä¸Šä¸‹æ–‡:\n{context}\n\n"
            
        prompt += """è¯·åˆ¶å®šä¸€ä¸ªåˆ†æ­¥éª¤çš„è®¡åˆ’æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚æ¯ä¸ªæ­¥éª¤åº”è¯¥æ¸…æ™°å¯æ‰§è¡Œï¼Œæ ¼å¼å¦‚ä¸‹:

æ­¥éª¤ 1: [æ­¥éª¤æè¿°]
- æ“ä½œç±»å‹: [æ“ä½œç±»å‹ï¼Œå¦‚æ–‡ä»¶æ“ä½œã€å±•æ¿æ“ä½œã€å†…å®¹ç”Ÿæˆç­‰]
- æ“ä½œ: [å…·ä½“æ“ä½œï¼Œå¦‚åˆ›å»ºè¯¾ç¨‹æ–‡ä»¶å¤¹ã€ç”Ÿæˆç¬”è®°ç­‰]
- éœ€è¦çš„ä¿¡æ¯: [æ­¤æ­¥éª¤éœ€è¦çš„ä¿¡æ¯]
- é¢„æœŸç»“æœ: [æ­¤æ­¥éª¤å®Œæˆåçš„ç»“æœ]

æ­¥éª¤ 2: ...

è¯·ç¡®ä¿æ­¥éª¤ä¹‹é—´æœ‰é€»è¾‘è¿è´¯æ€§ï¼Œä¸”æ¯ä¸ªæ­¥éª¤éƒ½éœ€è¦ç”¨æˆ·ç¡®è®¤æ‰èƒ½æ‰§è¡Œã€‚
å¦‚æœæŸäº›æ­¥éª¤éœ€è¦ä¸ç‰¹å®šå±•æ¿çš„ä¸“å®¶LLMåä½œï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
åŒæ—¶ï¼Œè¯·æä¾›ç¬¬ä¸€æ­¥å…·ä½“çš„æ‰§è¡Œå»ºè®®å’Œæ‰€éœ€çš„JSONå‘½ä»¤æ ¼å¼ã€‚

å¯¹äºéœ€è¦æ‰§è¡Œçš„æ“ä½œï¼Œè¯·æä¾›JSONæ ¼å¼çš„å‘½ä»¤ç¤ºä¾‹ï¼Œä¾‹å¦‚:
{"type": "file_operation", "action": "create_course_folder", "params": {"folder_name": "è¯¾ç¨‹åç§°"}}
"""
        
        # è°ƒç”¨LLM
        plan = self._call_llm(prompt)
        
        # è§£ææ­¥éª¤å’Œå‘½ä»¤
        steps = self._parse_steps(plan)
        commands = self._extract_commands_from_plan(plan)
        
        # è®¾ç½®å¤šæ­¥æ“ä½œä¸Šä¸‹æ–‡
        self.multi_step_context = {
            "active": True,
            "task": task_description,
            "plan": plan,
            "steps": steps,
            "commands": commands,
            "current_step": 0,
            "previous_result": None,
            "results": []
        }
        
        # è®°å½•æ“ä½œ
        self.add_operation("task_planned", {
            "task": task_description, 
            "steps_count": len(steps)
        })
        
        return {
            "plan": plan,
            "steps": steps,
            "commands": commands
        }
    
    def _extract_commands_from_plan(self, plan):
        """ä»è®¡åˆ’æ–‡æœ¬ä¸­æå–å‘½ä»¤"""
        import re
        import json
        
        commands = []
        
        # æŸ¥æ‰¾JSONæ ¼å¼çš„å‘½ä»¤
        json_pattern = r'({[\s\S]*?})'
        json_matches = re.findall(json_pattern, plan)
        
        for match in json_matches:
            try:
                cmd = json.loads(match)
                if isinstance(cmd, dict) and "type" in cmd and "action" in cmd:
                    commands.append(cmd)
            except:
                pass
        
        return commands
    
    def _parse_steps(self, plan):
        """ä»è®¡åˆ’æ–‡æœ¬ä¸­è§£æå‡ºæ­¥éª¤åˆ—è¡¨"""
        import re
        steps = []
        
        # åŒ¹é…"æ­¥éª¤ X: "æ¨¡å¼
        step_pattern = re.compile(r'æ­¥éª¤\s*(\d+):\s*(.*?)(?=æ­¥éª¤\s*\d+:|$)', re.DOTALL)
        matches = step_pattern.findall(plan)
        
        for step_num, step_content in matches:
            steps.append({
                "number": int(step_num),
                "description": step_content.strip()
            })
        
        return sorted(steps, key=lambda x: x["number"])
    
    def execute_step(self, step_description, previous_result=None, step_index=None):
        """
        æ‰§è¡Œä»»åŠ¡ä¸­çš„ä¸€ä¸ªæ­¥éª¤
        
        Args:
            step_description: æ­¥éª¤æè¿°
            previous_result: ä¸Šä¸€æ­¥çš„ç»“æœï¼ˆå¯é€‰ï¼‰
            step_index: æ­¥éª¤ç´¢å¼•ï¼ˆå¯é€‰ï¼Œç”¨äºç›´æ¥æ‰§è¡Œç‰¹å®šæ­¥éª¤ï¼‰
            
        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœå’Œå¯èƒ½çš„æ“ä½œå‘½ä»¤
        """
        # å¦‚æœæä¾›äº†æ­¥éª¤ç´¢å¼•ï¼Œæ›´æ–°å½“å‰æ­¥éª¤
        if step_index is not None and self.multi_step_context["active"]:
            if 0 <= step_index < len(self.multi_step_context["steps"]):
                self.multi_step_context["current_step"] = step_index
                step_description = self.multi_step_context["steps"][step_index]["description"]
            else:
                return {
                    "response": f"é”™è¯¯ï¼šæ­¥éª¤ç´¢å¼• {step_index} è¶…å‡ºèŒƒå›´ï¼ˆ0-{len(self.multi_step_context['steps'])-1}ï¼‰",
                    "command": None,
                    "error": True
                }
        
        prompt = f"ã€æ­¥éª¤æ‰§è¡Œã€‘å½“å‰æ­¥éª¤: {step_description}\n\n"
        
        # æ·»åŠ ä»»åŠ¡ä¸Šä¸‹æ–‡
        if self.multi_step_context["active"]:
            prompt += f"ä»»åŠ¡: {self.multi_step_context['task']}\n"
            prompt += f"å½“å‰æ­¥éª¤: {self.multi_step_context['current_step'] + 1}/{len(self.multi_step_context['steps'])}\n\n"
        
        if previous_result:
            prompt += f"ä¸Šä¸€æ­¥ç»“æœ:\n{previous_result}\n\n"
        elif self.multi_step_context["active"] and self.multi_step_context["previous_result"]:
            prompt += f"ä¸Šä¸€æ­¥ç»“æœ:\n{self.multi_step_context['previous_result']}\n\n"
            
        # æ·»åŠ æ‰€æœ‰ä¹‹å‰æ­¥éª¤çš„ç»“æœä½œä¸ºä¸Šä¸‹æ–‡
        if self.multi_step_context["active"] and self.multi_step_context["results"]:
            prompt += "ä¹‹å‰æ­¥éª¤çš„ç»“æœ:\n"
            for i, result in enumerate(self.multi_step_context["results"]):
                prompt += f"æ­¥éª¤ {i+1} ç»“æœ: {result}\n"
            prompt += "\n"
        
        prompt += """è¯·æ‰§è¡Œè¿™ä¸ªæ­¥éª¤å¹¶æä¾›ç»“æœã€‚å¦‚æœéœ€è¦é¢å¤–ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
å¦‚æœè¿™ä¸ªæ­¥éª¤éœ€è¦æ‰§è¡Œå…·ä½“æ“ä½œï¼Œè¯·æä¾›JSONæ ¼å¼çš„æ“ä½œå‘½ä»¤ã€‚
ä¾‹å¦‚: {"type": "navigation", "action": "next_page"}
æˆ–å¸¦å‚æ•°çš„: {"type": "file", "action": "upload_pdf", "params": {"course_id": "math101"}}

å¦‚æœè¿™ä¸ªæ­¥éª¤éœ€è¦ä¸ä¸“å®¶LLMåä½œï¼Œè¯·æä¾›åä½œè¯·æ±‚å‘½ä»¤:
{"type": "collaboration", "action": "consult_expert", "params": {"board_id": "board-123", "question": "åˆ†æè¿™ä¸ªPDFçš„ä¸»é¢˜"}}

è¯·ç¡®ä¿å‘½ä»¤æ ¼å¼æ­£ç¡®ï¼Œå¹¶æä¾›æ‰§è¡Œæ­¤æ­¥éª¤çš„è¯¦ç»†è¯´æ˜ã€‚"""
        
        # è°ƒç”¨LLM
        response = self._call_llm(prompt)
        
        # å°è¯•ä»å›å¤ä¸­æå–æ“ä½œå‘½ä»¤
        command = self._extract_command_json(response)
        
        # å¦‚æœæ˜¯å¤šæ­¥æ“ä½œä¸­çš„ä¸€æ­¥ï¼Œæ·»åŠ å¤šæ­¥æ“ä½œæ ‡è®°
        if self.multi_step_context["active"]:
            if command:
                # å¦‚æœå‘½ä»¤æ²¡æœ‰metadataå­—æ®µï¼Œæ·»åŠ ä¸€ä¸ª
                if "metadata" not in command:
                    command["metadata"] = {}
                
                # æ·»åŠ å¤šæ­¥æ“ä½œæ ‡è®°
                command["metadata"]["isMultiStep"] = True
                command["metadata"]["stepNumber"] = self.multi_step_context["current_step"] + 1
                command["metadata"]["totalSteps"] = len(self.multi_step_context["steps"])
                
                # æ›´æ–°å½“å‰æ­¥éª¤å’Œä¸Šä¸€æ­¥ç»“æœ
                clean_response = self._clean_response_json(response)
                self.multi_step_context["previous_result"] = clean_response
                
                # ä¿å­˜å½“å‰æ­¥éª¤ç»“æœ
                if self.multi_step_context["current_step"] < len(self.multi_step_context["results"]):
                    self.multi_step_context["results"][self.multi_step_context["current_step"]] = clean_response
                else:
                    self.multi_step_context["results"].append(clean_response)
                    
                self.multi_step_context["current_step"] += 1
        
        # è®°å½•æ“ä½œ
        self.add_operation("step_executed", {
            "step": step_description,
            "has_command": command is not None
        })
        
        return {
            "response": self._clean_response_json(response),
            "command": command,
            "step_index": self.multi_step_context["current_step"] - 1 if self.multi_step_context["active"] else None,
            "is_last_step": self.multi_step_context["active"] and self.multi_step_context["current_step"] >= len(self.multi_step_context["steps"])
        }
    
    def continue_multi_step_task(self):
        """ç»§ç»­æ‰§è¡Œå¤šæ­¥éª¤ä»»åŠ¡çš„ä¸‹ä¸€æ­¥"""
        if not self.multi_step_context["active"]:
            return {
                "response": "å½“å‰æ²¡æœ‰æ´»åŠ¨çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚",
                "command": None
            }
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ‰€æœ‰æ­¥éª¤
        if self.multi_step_context["current_step"] >= len(self.multi_step_context["steps"]):
            # é‡ç½®å¤šæ­¥æ“ä½œä¸Šä¸‹æ–‡
            self.multi_step_context["active"] = False
            
            return {
                "response": "å¤šæ­¥éª¤ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆã€‚",
                "command": None
            }
        
        # è·å–å½“å‰æ­¥éª¤
        current_step = self.multi_step_context["steps"][self.multi_step_context["current_step"]]
        previous_result = self.multi_step_context["previous_result"]
        
        # æ‰§è¡Œå½“å‰æ­¥éª¤
        result = self.execute_step(current_step["description"], previous_result)
        
        # æ·»åŠ å¤šæ­¥æ“ä½œä¸Šä¸‹æ–‡ä¿¡æ¯
        result["multi_step_context"] = {
            "current_step": self.multi_step_context["current_step"],
            "total_steps": len(self.multi_step_context["steps"]),
            "is_last_step": self.multi_step_context["current_step"] >= len(self.multi_step_context["steps"])
        }
        
        return result
    
    def execute_task(self, task_description, context=None):
        """
        æ‰§è¡Œå®Œæ•´ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§„åˆ’å’Œæ‰§è¡Œæ‰€æœ‰æ­¥éª¤
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        # é¦–å…ˆè§„åˆ’ä»»åŠ¡
        plan_result = self.plan_multi_step_task(task_description, context)
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        results = []
        
        # é€æ­¥æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
        current_step = 0
        total_steps = len(self.multi_step_context["steps"])
        
        while current_step < total_steps:
            # è·å–å½“å‰æ­¥éª¤
            step = self.multi_step_context["steps"][current_step]
            
            # æ‰§è¡Œå½“å‰æ­¥éª¤
            step_result = self.execute_step(step["description"])
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            results.append({
                "step_number": current_step + 1,
                "description": step["description"],
                "response": step_result["response"],
                "command": step_result["command"]
            })
            
            # æ›´æ–°å½“å‰æ­¥éª¤
            current_step = self.multi_step_context["current_step"]
        
        # ç”Ÿæˆä»»åŠ¡æ€»ç»“
        summary_prompt = f"ã€ä»»åŠ¡æ€»ç»“ã€‘å·²å®Œæˆä»»åŠ¡: {task_description}\n\n"
        summary_prompt += "å„æ­¥éª¤æ‰§è¡Œç»“æœ:\n"
        
        for i, result in enumerate(results):
            summary_prompt += f"æ­¥éª¤ {i+1}: {result['description']}\n"
            summary_prompt += f"ç»“æœ: {result['response'][:100]}...\n\n"
        
        summary_prompt += "è¯·æä¾›ä»»åŠ¡æ‰§è¡Œçš„æ€»ç»“ï¼ŒåŒ…æ‹¬å®Œæˆæƒ…å†µã€ä¸»è¦æˆæœå’Œå¯èƒ½çš„åç»­è¡ŒåŠ¨ã€‚"
        
        # è°ƒç”¨LLMç”Ÿæˆæ€»ç»“
        summary = self._call_llm(summary_prompt)
        
        # é‡ç½®å¤šæ­¥æ“ä½œä¸Šä¸‹æ–‡
        self.multi_step_context["active"] = False
        
        # è®°å½•æ“ä½œ
        self.add_operation("task_completed", {
            "task": task_description,
            "steps_executed": len(results)
        })
        
        return {
            "task": task_description,
            "steps": results,
            "summary": summary
        }
    
    def stream_call_llm(self, prompt, callback=None):
        """
        ä½¿ç”¨æµå¼æ–¹å¼è°ƒç”¨LLM API
        
        Args:
            prompt: æç¤ºæ–‡æœ¬
            callback: å›è°ƒå‡½æ•°ï¼Œç”¨äºå¤„ç†æµå¼è¾“å‡ºçš„æ¯ä¸ªæ•°æ®å—
            
        Returns:
            å®Œæ•´å“åº”æ–‡æœ¬
        """
        if not QWEN_API_KEY:
            logger.error("æœªé…ç½®QWEN_API_KEY")
            raise ValueError("æœªé…ç½®QWEN_API_KEY")
        
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹çš„APIè°ƒç”¨
            from openai import OpenAI
            
            client = OpenAI(
                api_key=QWEN_API_KEY,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            )
            
            # è·å–å†å²å¯¹è¯
            conversation_history = conversation_manager.get_conversation(self.session_id, "global")
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
            system_message = "ä½ æ˜¯WhatNoteåº”ç”¨çš„ç®¡å®¶LLMï¼Œè´Ÿè´£ååŠ©ç”¨æˆ·ç®¡ç†æ–‡ä»¶å’Œå±•æ¿ã€‚"
            messages.append({"role": "system", "content": system_message})
            
            # æ·»åŠ å¯¹è¯å†å²
            for msg in conversation_history[-8:]:  # æœ€å¤šå–8æ¡å†å²è®°å½•
                role = msg.get("role")
                content = msg.get("content")
                if role and content and role in ["user", "assistant"]:
                    messages.append({"role": role, "content": content})
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": prompt})
            
            # æµå¼è°ƒç”¨
            completion = client.chat.completions.create(
                model="qwen-plus",
                messages=messages,
                stream=True,
                stream_options={"include_usage": True}
            )
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_response = ""
            
            # å¤„ç†æµå¼å“åº”
            for chunk in completion:
                if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                    content_chunk = chunk.choices[0].delta.content
                    full_response += content_chunk
                    
                    # å¦‚æœæœ‰å›è°ƒå‡½æ•°ï¼Œè°ƒç”¨å®ƒ
                    if callback and callable(callback):
                        callback(content_chunk)
            
            # å°†ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤æ·»åŠ åˆ°å†å²è®°å½•
            conversation_manager.add_message(self.session_id, "global", "user", prompt)
            conversation_manager.add_message(self.session_id, "global", "assistant", full_response)
            
            # è®°å½•äº¤äº’æ—¥å¿—
            LLMLogger.log_interaction(
                llm_type="butler_stream",
                query=prompt,
                response=full_response,
                command=None,
                metadata={
                    "streaming": True,
                    "session_id": self.session_id
                }
            )
            
            logger.info(f"ç®¡å®¶LLMæµå¼è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(full_response)}")
            return full_response
        
        except Exception as e:
            error_msg = f"æµå¼è°ƒç”¨å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            if callback:
                callback(f"é”™è¯¯: {str(e)}")
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            LLMLogger.log_interaction(
                llm_type="butler_stream",
                query=prompt,
                response=error_msg,
                command=None,
                metadata={
                    "error": str(e),
                    "streaming": True,
                    "session_id": self.session_id
                }
            )
            
            return error_msg
    
    def _call_llm(self, prompt):
        """å†…éƒ¨æ–¹æ³•ï¼šè°ƒç”¨LLM API"""
        if not QWEN_API_KEY:
            logger.error("æœªé…ç½®QWEN_API_KEY")
            return "APIè°ƒç”¨é”™è¯¯ï¼šæœªé…ç½®APIå¯†é’¥"
            
        # è·å–å†å²å¯¹è¯
        conversation_history = conversation_manager.get_conversation(self.session_id, "global")
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        conversation_manager.add_message(self.session_id, "global", "user", prompt)
        
        try:
            # å‡†å¤‡APIè¯·æ±‚ - ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼
            import requests
            
            url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {QWEN_API_KEY}",
                "Content-Type": "application/json"
            }
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæœ€å¤šå–æœ€è¿‘10æ¡
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
            system_message = "ä½ æ˜¯WhatNoteåº”ç”¨çš„ç®¡å®¶LLMï¼Œè´Ÿè´£ååŠ©ç”¨æˆ·ç®¡ç†æ–‡ä»¶å’Œå±•æ¿ã€‚"
            messages.append({"role": "system", "content": system_message})
            
            # æ·»åŠ å¯¹è¯å†å²
            for msg in conversation_history[-8:]:  # æœ€å¤šå–8æ¡å†å²è®°å½•
                role = msg.get("role")
                content = msg.get("content")
                if role and content and role in ["user", "assistant"]:
                    messages.append({"role": role, "content": content})
            
            # ç¡®ä¿æœ€åä¸€æ¡æ˜¯å½“å‰ç”¨æˆ·æ¶ˆæ¯
            if not (len(messages) >= 2 and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                messages.append({"role": "user", "content": prompt})
            
            data = {
                "model": "qwen-max",  # ä½¿ç”¨æ›´é«˜çº§çš„æ¨¡å‹
                "messages": messages,
                "temperature": 0.7
            }
            
            # è®°å½•è°ƒè¯•ä¿¡æ¯
            logger.info(f"å‘é€APIè¯·æ±‚ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            # å‘é€è¯·æ±‚ - é…ç½®ä»£ç†è®¾ç½®ä»¥é¿å…è¿æ¥é—®é¢˜
            start_time = time.time()
            proxies = {'http': None, 'https': None}
            response = requests.post(url, headers=headers, json=data, timeout=API_TIMEOUT, proxies=proxies)
            response.raise_for_status()
            
            result = response.json()
            response_content = result["choices"][0]["message"]["content"]
            
            # è®¡ç®—APIè°ƒç”¨è€—æ—¶
            end_time = time.time()
            duration = end_time - start_time
            
            # è®°å½•LLMäº¤äº’æ—¥å¿—
            LLMLogger.log_interaction(
                llm_type="butler",
                query=prompt,
                response=response_content,
                metadata={
                    "session_id": self.session_id,
                    "duration": duration,
                    "token_count": result.get("usage", {}).get("total_tokens", 0)
                }
            )
            
            # æ·»åŠ åŠ©æ‰‹å›å¤
            conversation_manager.add_message(
                self.session_id, 
                "global", 
                "assistant", 
                response_content
            )
            
            return response_content
            
        except Exception as e:
            logger.error(f"ç®¡å®¶LLM APIè°ƒç”¨å¤±è´¥: {str(e)}")
            error_msg = f"APIè°ƒç”¨é”™è¯¯: {str(e)}"
            
            # è®°å½•é”™è¯¯å›å¤
            conversation_manager.add_message(
                self.session_id, 
                "global", 
                "assistant", 
                error_msg
            )
            
            # è®°å½•LLMäº¤äº’é”™è¯¯æ—¥å¿—
            LLMLogger.log_interaction(
                llm_type="butler",
                query=prompt,
                response=error_msg,
                metadata={
                    "session_id": self.session_id,
                    "error": str(e)
                }
            )
            
            return error_msg
    
    def _execute_function_call(self, command):
        """æ‰§è¡Œfunction call"""
        action = command.get("action")
        params = command.get("params", {})
        command_type = command.get("type")
        
        logger.info(f"ğŸ”§ [BUTLER] æ‰§è¡Œfunction call: {action}")
        
        # æ–‡ä»¶æ“ä½œ
        if command_type == "file_operation":
            return self._handle_file_operation(action, params)
        
        # å±•æ¿æ“ä½œ
        elif command_type == "board_operation":
            return self._handle_board_operation(action, params)
        
        # ç³»ç»ŸæŸ¥è¯¢
        elif command_type == "system_query":
            return self._handle_system_query(action, params)
        
        # ä¸“å®¶å’¨è¯¢
        elif command_type == "expert_consultation":
            return self._handle_expert_consultation(action, params)
        
        # ä»»åŠ¡æ“ä½œ
        elif command_type == "task":
            return self._handle_task_operation(action, params)
        
        else:
            return f"æœªçŸ¥çš„æ“ä½œç±»å‹: {command_type}"
    
    def _handle_file_operation(self, action, params):
        """å¤„ç†æ–‡ä»¶æ“ä½œ"""
        if action == "create_course_folder":
            folder_name = params.get("folder_name")
            if not folder_name:
                return "é”™è¯¯: ç¼ºå°‘folder_nameå‚æ•°"
            
            # è¿™é‡Œéœ€è¦è°ƒç”¨å®é™…çš„APIæ¥åˆ›å»ºè¯¾ç¨‹æ–‡ä»¶å¤¹
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
            return f"è¯¾ç¨‹æ–‡ä»¶å¤¹ '{folder_name}' åˆ›å»ºæˆåŠŸ"
        
        elif action == "get_file_list":
            # è·å–æ–‡ä»¶åˆ—è¡¨
            file_structure = self.butler_log.get("file_structure", {})
            uploaded_files = file_structure.get("uploaded_files", [])
            file_list = [f["filename"] for f in uploaded_files]
            return f"å½“å‰æ–‡ä»¶åˆ—è¡¨: {', '.join(file_list) if file_list else 'æ— æ–‡ä»¶'}"
        
        elif action == "delete_file":
            filename = params.get("filename")
            if not filename:
                return "é”™è¯¯: ç¼ºå°‘filenameå‚æ•°"
            return f"æ–‡ä»¶ '{filename}' åˆ é™¤æ“ä½œå·²æäº¤"
        
        else:
            return f"æœªçŸ¥çš„æ–‡ä»¶æ“ä½œ: {action}"
    
    def _handle_board_operation(self, action, params):
        """å¤„ç†å±•æ¿æ“ä½œ"""
        if action == "create_board":
            board_name = params.get("board_name")
            course_folder = params.get("course_folder")
            if not board_name:
                return "é”™è¯¯: ç¼ºå°‘board_nameå‚æ•°"
            return f"å±•æ¿ '{board_name}' åˆ›å»ºæˆåŠŸ"
        
        elif action == "list_boards":
            boards = self.butler_log.get("boards", {})
            board_list = list(boards.keys())
            return f"å½“å‰å±•æ¿åˆ—è¡¨: {', '.join(board_list) if board_list else 'æ— å±•æ¿'}"
        
        elif action == "get_board_info":
            board_id = params.get("board_id")
            if not board_id:
                return "é”™è¯¯: ç¼ºå°‘board_idå‚æ•°"
            
            boards = self.butler_log.get("boards", {})
            board_info = boards.get(board_id, {})
            return f"å±•æ¿ {board_id} ä¿¡æ¯: {board_info}"
        
        else:
            return f"æœªçŸ¥çš„å±•æ¿æ“ä½œ: {action}"
    
    def _handle_system_query(self, action, params):
        """å¤„ç†ç³»ç»ŸæŸ¥è¯¢"""
        if action == "get_app_state":
            file_structure = self.butler_log.get("file_structure_summary", {})
            return f"åº”ç”¨çŠ¶æ€: è¯¾ç¨‹æ–‡ä»¶å¤¹ {file_structure.get('course_folders', 0)} ä¸ª, å±•æ¿ {file_structure.get('boards', 0)} ä¸ª, æ–‡ä»¶ {file_structure.get('uploaded_files', 0)} ä¸ª"
        
        elif action == "get_recent_operations":
            operations = self.butler_log.get("recent_operations", [])
            recent = operations[-5:] if operations else []
            op_summary = [f"{op['type']} ({op['timestamp'][:19]})" for op in recent]
            return f"æœ€è¿‘æ“ä½œ: {', '.join(op_summary) if op_summary else 'æ— æ“ä½œè®°å½•'}"
        
        else:
            return f"æœªçŸ¥çš„ç³»ç»ŸæŸ¥è¯¢: {action}"
    
    def _handle_expert_consultation(self, action, params):
        """å¤„ç†ä¸“å®¶å’¨è¯¢"""
        if action == "consult_expert":
            board_id = params.get("board_id")
            question = params.get("question")
            
            if not board_id or not question:
                return "é”™è¯¯: ç¼ºå°‘board_idæˆ–questionå‚æ•°"
            
            return self.consult_expert(board_id, question)
        
        else:
            return f"æœªçŸ¥çš„ä¸“å®¶å’¨è¯¢æ“ä½œ: {action}"
    
    def _handle_task_operation(self, action, params):
        """å¤„ç†ä»»åŠ¡æ“ä½œ"""
        if action == "plan_task":
            task = params.get("task")
            if not task:
                return "é”™è¯¯: ç¼ºå°‘taskå‚æ•°"
            
            plan_result = self.plan_multi_step_task(task)
            return f"ä»»åŠ¡è§„åˆ’å®Œæˆ: {len(plan_result['steps'])} ä¸ªæ­¥éª¤"
        
        elif action == "execute_step":
            if not self.multi_step_context.get("active"):
                return "é”™è¯¯: æ²¡æœ‰æ´»è·ƒçš„å¤šæ­¥ä»»åŠ¡"
            
            return self.continue_multi_step_task()
        
        else:
            return f"æœªçŸ¥çš„ä»»åŠ¡æ“ä½œ: {action}"

# å…¨å±€å•ä¾‹
butler_llm = ButlerLLM()
